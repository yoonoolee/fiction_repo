{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# LLM-based Filtering for Reddit Stories\n\nUses Groq API with Llama 3.1-8B to classify stories as KEEP (ridiculous, humorous, absurd) or REMOVE (depressing, serious, harmful)\n\nFeatures:\n- Resumable (saves progress every 100 rows)\n- Cloud-based inference (won't crash your computer!)\n- Progress tracking\n- Free tier: 1,000 requests/day, 30 requests/minute\n- Estimated time: ~3 days for 2,611 stories"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install groq (uncomment if needed)\n# !pip install groq"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Imports\nimport pandas as pd\nfrom pathlib import Path\nimport sys\nfrom tqdm import tqdm\nimport json\nimport time\nfrom groq import Groq\n\n# Add parent directory to path\nsys.path.append(str(Path.cwd().parent))\nfrom config import RAW_DATA_DIR\n\nprint(f\"Data directory: {RAW_DATA_DIR}\")\n\n# Initialize Groq client\n# Get your free API key from: https://console.groq.com/keys\nGROQ_API_KEY = input(\"Enter your Groq API key: \")\nclient = Groq(api_key=GROQ_API_KEY)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup Groq API\n\n1. Get your free API key: https://console.groq.com/keys\n2. Enter it in the cell above\n3. Free tier: 1,000 requests/day, 30 requests/minute"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test Groq connection\ntry:\n    response = client.chat.completions.create(\n        model=\"llama-3.1-8b-instant\",\n        messages=[{\"role\": \"user\", \"content\": 'Say \"OK\" if you can read this.'}],\n        max_tokens=10\n    )\n    print(\"‚úÖ Groq is working!\")\n    print(f\"Response: {response.choices[0].message.content}\")\nexcept Exception as e:\n    print(f\"‚ùå Error: {e}\")\n    print(\"Make sure you entered a valid API key\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the confession dataset\nconfession_file = RAW_DATA_DIR / \"reddit_confession.parquet\"\n    \nif not confession_file.exists():\n    raise FileNotFoundError(f\"Cannot find confession data. Looking for: {confession_file}\")\n\ndf = pd.read_parquet(confession_file)\nprint(f\"Loaded {len(df)} rows\")\nprint(f\"Columns: {list(df.columns)}\")\nprint(f\"\\nFirst row:\")\nprint(df.iloc[0])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Classification Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def classify_story(text, title=\"\"):\n    \"\"\"\n    Classify a story as KEEP or REMOVE using Groq API.\n    \n    Args:\n        text: Story text\n        title: Story title (optional)\n    \n    Returns:\n        'KEEP' or 'REMOVE'\n    \"\"\"\n    \n    prompt = f\"\"\"You are a content filter. Classify this Reddit story as KEEP or REMOVE.\n\nREMOVE stories that contain:\n- Death, dying, serious illness, hospitals, medical emergencies\n- Violence, abuse, assault, harm to people or animals\n- Suicide, self-harm, depression, serious mental health issues\n- Drug addiction, alcoholism, substance abuse, DUI, rehab\n- Guilt, regret, shame, \"still think about it\", feeling terrible\n- Cheating, divorce, serious relationship problems\n- Getting people in trouble, suspended, fired, arrested\n- Trauma, PTSD, haunting memories\n- Anything genuinely sad, upsetting, or disturbing\n- Serious consequences or people getting hurt\n\nKEEP stories that are:\n- Genuinely funny, silly, or absurdly ridiculous\n- Harmless embarrassing moments\n- Lighthearted chaos with no real harm\n- Weird situations that are entertaining\n- No guilt, no regret, no one gets hurt\n\nExamples:\n\nStory: \"I shot up in the hospital while my mom was on life support\"\nClassification: REMOVE (death, hospital, serious)\n\nStory: \"I played pedestrian chicken while driving drugged\"\nClassification: REMOVE (drugs, DUI, could have killed people)\n\nStory: \"I outed a teacher, got someone else suspended, still think about it and feel terrible\"\nClassification: REMOVE (guilt, regret, got someone suspended, serious consequences)\n\nStory: \"I accidentally walked into the wrong apartment and sat on someone's couch before realizing\"\nClassification: KEEP (harmless funny mistake)\n\nNow classify this story:\n\nTitle: {title}\nStory: {text}\n\nThink step by step:\n1. Does anyone get seriously hurt or could have been hurt?\n2. Is there guilt, regret, or serious consequences?\n3. Is it genuinely funny and lighthearted, or is it sad/disturbing?\n\nAnswer with ONLY the word: KEEP or REMOVE\"\"\"\n    \n    try:\n        response = client.chat.completions.create(\n            model=\"llama-3.1-8b-instant\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=0.0,\n            max_tokens=20\n        )\n        \n        result = response.choices[0].message.content.strip().upper()\n        \n        if 'KEEP' in result:\n            return 'KEEP'\n        elif 'REMOVE' in result:\n            return 'REMOVE'\n        else:\n            return 'REMOVE'  # Default to REMOVE\n            \n    except Exception as e:\n        print(f\"Error classifying story: {e}\")\n        return 'REMOVE'"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Classification on Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(20, 40):\n",
    "    row = df.iloc[idx]\n",
    "    title = row.get('title', '')\n",
    "    text = row.get('text', '')\n",
    "    \n",
    "    result = classify_story(text, title)\n",
    "    \n",
    "    print(f\"Row {idx}:\")\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Result: {result}\")\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Classification on Full Dataset (Resumable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup for resumable processing\ncheckpoint_file = RAW_DATA_DIR / \"llm_filter_checkpoint.json\"\ncheckpoint_interval = 100  # Save every 100 rows\n\n# Load checkpoint if exists\nif checkpoint_file.exists():\n    with open(checkpoint_file, 'r') as f:\n        checkpoint_data = json.load(f)\n    classifications = checkpoint_data['classifications']\n    start_idx = checkpoint_data['last_processed'] + 1\n    print(f\"Resuming from row {start_idx} (found checkpoint)\")\nelse:\n    classifications = {}\n    start_idx = 0\n    print(\"Starting from beginning\")\n\n# Add classification column if not exists\nif 'llm_label' not in df.columns:\n    df['llm_label'] = None\n\n# Process all rows with rate limiting\nprint(f\"\\nProcessing {len(df) - start_idx} rows...\")\nprint(f\"Rate limit: 30 requests/minute (~2 second delay between requests)\")\nprint(f\"Estimated time: ~{((len(df) - start_idx) * 2) / 60:.1f} minutes\\n\")\n\ntry:\n    for idx in tqdm(range(start_idx, len(df)), desc=\"Classifying stories\"):\n        row = df.iloc[idx]\n        title = row.get('title', '')\n        text = row.get('text', '')\n        \n        # Classify\n        label = classify_story(text, title)\n        classifications[idx] = label\n        df.at[idx, 'llm_label'] = label\n        \n        # Rate limiting: 30 req/min = 2 seconds between requests\n        time.sleep(2)\n        \n        # Save checkpoint every N rows\n        if (idx + 1) % checkpoint_interval == 0:\n            checkpoint_data = {\n                'last_processed': idx,\n                'classifications': classifications\n            }\n            with open(checkpoint_file, 'w') as f:\n                json.dump(checkpoint_data, f)\n            \nexcept KeyboardInterrupt:\n    print(\"\\n\\nInterrupted! Saving checkpoint...\")\n    checkpoint_data = {\n        'last_processed': idx - 1,\n        'classifications': classifications\n    }\n    with open(checkpoint_file, 'w') as f:\n        json.dump(checkpoint_data, f)\n    print(f\"Checkpoint saved. Processed up to row {idx}\")\n    \nprint(\"\\n‚úÖ Classification complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count results\n",
    "keep_count = (df['llm_label'] == 'KEEP').sum()\n",
    "remove_count = (df['llm_label'] == 'REMOVE').sum()\n",
    "\n",
    "print(f\"\\n=== Classification Results ===\")\n",
    "print(f\"Total stories: {len(df)}\")\n",
    "print(f\"KEEP: {keep_count} ({keep_count/len(df)*100:.1f}%)\")\n",
    "print(f\"REMOVE: {remove_count} ({remove_count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Show some examples of KEEP stories\n",
    "print(\"\\n=== Sample KEEP Stories ===\")\n",
    "keep_stories = df[df['llm_label'] == 'KEEP'].head(5)\n",
    "for idx, row in keep_stories.iterrows():\n",
    "    print(f\"\\nTitle: {row.get('title', 'N/A')}\")\n",
    "    print(f\"Text: {row.get('text', '')[:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Filtered Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Filter to only KEEP stories\nfiltered_df = df[df['llm_label'] == 'KEEP'].copy()\n\n# Drop the classification column (optional)\nfiltered_df = filtered_df.drop(columns=['llm_label'])\n\n# Save\noutput_file = RAW_DATA_DIR / \"reddit_confession_filtered.parquet\"\nfiltered_df.to_parquet(output_file, index=False)\n\nprint(f\"\\n‚úÖ Saved {len(filtered_df)} filtered stories to:\")\nprint(f\"   {output_file}\")\n\n# Clean up checkpoint\nif checkpoint_file.exists():\n    checkpoint_file.unlink()\n    print(\"\\nüßπ Cleaned up checkpoint file\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}