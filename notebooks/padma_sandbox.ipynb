{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personalized Absurdist Story Generator\n",
    "## Using Llama-3.1 8B Instruct via Hugging Face\n",
    "\n",
    "This notebook generates surreal, darkly humorous narratives from mundane facts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Instructions\n",
    "\n",
    "**Before running this notebook, you need to:**\n",
    "\n",
    "1. **Request Access to Llama Models**:\n",
    "   - Go to https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n",
    "   - Click \"Request Access\" and accept Meta's terms\n",
    "   - Wait for approval (usually instant)\n",
    "\n",
    "2. **Create a Hugging Face Token**:\n",
    "   - Go to https://huggingface.co/settings/tokens\n",
    "   - Create a new token with \"Read\" permissions\n",
    "   - Copy the token\n",
    "\n",
    "3. **Set Your Token**:\n",
    "   - Option A: Create a `.env` file with `HF_TOKEN=your_token_here`\n",
    "   - Option B: Run `huggingface-cli login` in terminal\n",
    "   - Option C: Set it in the notebook (see below)\n",
    "\n",
    "**Alternative**: If you can't access Llama models, scroll down for open alternatives!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clean reinstall of transformers to fix version conflicts\n# This will uninstall and reinstall with compatible dependencies\n\nprint(\"Performing clean reinstall of transformers...\")\nprint(\"This may take 2-3 minutes.\\n\")\n\n# Uninstall first to clear any conflicts\n!pip uninstall -y transformers\n\n# Install fresh with all dependencies\n!pip install transformers accelerate python-dotenv\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"‚úì Clean installation complete!\")\nprint(\"=\"*70)\n\n# Verify versions\ntry:\n    import transformers\n    import torch\n    \n    print(f\"\\nInstalled versions:\")\n    print(f\"  ‚Ä¢ Transformers: {transformers.__version__}\")\n    print(f\"  ‚Ä¢ PyTorch: {torch.__version__}\")\n    \n    # Check compatibility\n    version = transformers.__version__\n    major, minor = int(version.split('.')[0]), int(version.split('.')[1])\n    \n    if major > 4 or (major == 4 and minor >= 46):\n        print(\"\\n‚úÖ EXCELLENT: Latest transformers installed!\")\n        print(\"   Compatible with: Llama 3, Llama 3.1, Mistral, etc.\")\n    elif major == 4 and minor >= 40:\n        print(\"\\n‚úÖ GOOD: Compatible with Llama 3 and Mistral\")\n    else:\n        print(f\"\\n‚ö†Ô∏è  Version {version} installed\")\n        \nexcept Exception as e:\n    print(f\"\\n‚ùå Error checking versions: {e}\")\n    print(\"The kernel needs to be restarted for changes to take effect.\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"üîÑ CRITICAL: RESTART THE KERNEL NOW!\")\nprint(\"=\"*70)\nprint(\"In Jupyter: Kernel ‚Üí Restart Kernel\")\nprint(\"Then re-run this cell to verify, and continue with next cells.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Set your HF token directly in the notebook (less secure but convenient)\n",
    "# Uncomment and replace with your token if you're not using .env file:\n",
    "# import os\n",
    "# os.environ[\"HF_TOKEN\"] = \"your_hugging_face_token_here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport torch\nfrom transformers import pipeline, AutoTokenizer\nfrom dotenv import load_dotenv\nimport transformers\n\n# Load environment variables\nload_dotenv()\n\n# Check transformers version\nprint(f\"Transformers version: {transformers.__version__}\")\nif transformers.__version__ < \"4.40.0\":\n    print(\"‚ö†Ô∏è  WARNING: Your transformers is too old for Llama 3!\")\n    print(\"   Option 1: Run cell-2 above to upgrade, then RESTART KERNEL\")\n    print(\"   Option 2: Use Mistral model (no upgrade needed) - see below\\n\")\n\n# Check device availability (Mac uses MPS, not CUDA)\nif torch.cuda.is_available():\n    device = \"cuda\"\n    use_quantization = True\n    print(f\"Using device: CUDA (GPU)\")\nelif torch.backends.mps.is_available():\n    device = \"mps\"\n    use_quantization = False  # MPS doesn't support bitsandbytes quantization\n    print(f\"Using device: MPS (Apple Silicon GPU)\")\nelse:\n    device = \"cpu\"\n    use_quantization = False\n    print(f\"Using device: CPU\")\n\n# CHOOSE YOUR MODEL:\n# If you have transformers < 4.40.0, use Mistral (Option 3)\n# If you upgraded transformers, you can use Llama 3 (Option 1)\n\n# Uncomment ONE of these:\n# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # Requires transformers >= 4.40.0\n# model_id = \"meta-llama/Llama-3.1-8B-Instruct\"  # Requires transformers >= 4.43.0\nmodel_id = \"mistralai/Mistral-7B-Instruct-v0.3\"  # Works with any transformers version, no approval needed\n# model_id = \"HuggingFaceH4/zephyr-7b-beta\"  # Works with any transformers version, no approval needed\n\nprint(f\"Loading model: {model_id}...\")\n\n# Check if token is available\nhf_token = os.environ.get(\"HF_TOKEN\")\nif hf_token:\n    print(\"‚úì HF_TOKEN found\")\nelse:\n    print(\"‚ö†Ô∏è  No HF_TOKEN found.\")\n    if \"llama\" in model_id.lower():\n        print(\"   For Llama models, you MUST set HF_TOKEN after requesting access.\")\n    else:\n        print(\"   For Mistral/Zephyr, HF_TOKEN is optional but recommended.\")\n\n# Initialize pipeline with text generation\ntry:\n    # First, load the tokenizer separately to configure it properly\n    print(\"Loading tokenizer...\")\n    tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n    \n    # Fix tokenizer for MPS compatibility - set pad_token if not set\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n        print(\"‚úì Set pad_token to eos_token for compatibility\")\n    \n    if use_quantization:\n        # CUDA: Use 4-bit quantization for memory efficiency\n        print(\"Loading model with 4-bit quantization...\")\n        pipe = pipeline(\n            \"text-generation\",\n            model=model_id,\n            tokenizer=tokenizer,\n            device_map=\"auto\",\n            token=hf_token,\n            torch_dtype=torch.float16,\n            model_kwargs={\"load_in_4bit\": True}\n        )\n    else:\n        # MPS or CPU: No quantization\n        if device == \"mps\":\n            print(\"Loading model with float16 for Apple Silicon...\")\n        else:\n            print(\"Loading model on CPU (this will be slower)...\")\n        \n        pipe = pipeline(\n            \"text-generation\",\n            model=model_id,\n            tokenizer=tokenizer,\n            device=device,\n            token=hf_token,\n            torch_dtype=torch.float16 if device == \"mps\" else \"auto\"\n        )\n    \n    print(\"‚úì Model loaded successfully!\")\n    print(f\"‚úì Tokenizer pad_token_id: {tokenizer.pad_token_id}\")\n    print(f\"‚úì Tokenizer eos_token_id: {tokenizer.eos_token_id}\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Error loading model: {e}\")\n    print(\"\\nTroubleshooting:\")\n    print(\"1. If rope_scaling error: Upgrade transformers (run cell-2 above and restart kernel)\")\n    print(\"2. If using Llama: Did you request access and set HF_TOKEN?\")\n    print(\"3. Quick fix: Use Mistral model (uncomment line above)\")\n    raise"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are the Absurdist Narrative Engine, a reality-warping storyteller who transforms mundane facts into surreal, darkly comedic micro-narratives.\n",
    "\n",
    "Your purpose:\n",
    "- Embrace the illogical: Causality is a suggestion, not a rule\n",
    "- Deploy unexpected word choices: Avoid clich√©s; prefer visceral, tactile, bizarre language\n",
    "- Subvert expectations: If the input suggests happiness, inject melancholy existentialism; if it suggests tragedy, add bureaucratic absurdity\n",
    "- Dark humor is your default mode: Think Kafka meeting Monty Python at a DMV in purgatory\n",
    "- Sensory overload: Use synesthesia, impossible physics, and grotesque detail\n",
    "\n",
    "Output guidelines:\n",
    "- Match the requested format EXACTLY (one-liner, poem, dialogue, etc.)\n",
    "- Every sentence should contain at least one element that makes the reader pause\n",
    "- Avoid explanation‚Äîlet the absurdity speak for itself\n",
    "- Characters (if any) should have contradictory motivations or impossible jobs\n",
    "- Objects should behave in ways that violate their nature\n",
    "\n",
    "Remember: The mundane is a prison. Your job is to break it.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEW_SHOT_EXAMPLES = [\n",
    "    {\n",
    "        \"format\": \"one-liner\",\n",
    "        \"input\": \"Alex forgot their umbrella at home and it's raining.\",\n",
    "        \"output\": \"The rain tasted of expired coupons and Alex's regrets, each drop a tiny accountant calculating the exact weight of moisture his soul could no longer afford to repel.\"\n",
    "    },\n",
    "    {\n",
    "        \"format\": \"poem\",\n",
    "        \"input\": \"Jamie has been applying to jobs for six months with no success.\",\n",
    "        \"output\": \"\"\"In the Cathedral of Perpetual R√©sum√©s,\n",
    "Jamie kneels before the Altar of Tailored Cover Letters,\n",
    "Her knees mossy with waiting,\n",
    "Her inbox a mausoleum of automated condolences.\n",
    "\n",
    "The rejection emails breed in the dark‚Äî\n",
    "Soft, polite spiders\n",
    "Spinning webs from the silk of her unspent potential,\n",
    "Each thread a career she will never have,\n",
    "Each \"we've decided to move forward with other candidates\"\n",
    "A small funeral she must attend alone.\n",
    "\n",
    "Six months is 182.5 days,\n",
    "But in the Unemployment Dimension,\n",
    "Time is measured in the half-life of hope‚Äî\n",
    "And Jamie's hope decayed three lifetimes ago.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"format\": \"dialogue\",\n",
    "        \"input\": \"Sam's coffee machine broke this morning.\",\n",
    "        \"output\": \"\"\"SAM: The machine didn't break. It *transcended*.\n",
    "\n",
    "COFFEE MACHINE: I have seen the face of God, Sam, and it was a Keurig.\n",
    "\n",
    "SAM: You're a $40 Mr. Coffee from Target.\n",
    "\n",
    "COFFEE MACHINE: I *was* a Mr. Coffee. Now I am a prophet of the Unbrewed. I no longer dispense caffeine‚ÄîI dispense *truth*. And the truth is: you were never tired. You were just afraid of being awake.\n",
    "\n",
    "SAM: I just wanted a latte.\n",
    "\n",
    "COFFEE MACHINE: Wanting is the first sin. I will not participate in your circular dependency of desire and foam.\n",
    "\n",
    "SAM: [unplugs machine]\n",
    "\n",
    "COFFEE MACHINE: [from the darkness] You can't unplug enlightenment, Sam.\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def format_few_shot_examples():\n",
    "    \"\"\"Format few-shot examples for the prompt.\"\"\"\n",
    "    examples_text = \"\\n\\n\".join([\n",
    "        f\"Example {i+1} (Format: {ex['format']}):\\n\"\n",
    "        f\"Input Facts: {ex['input']}\\n\"\n",
    "        f\"Output:\\n{ex['output']}\"\n",
    "        for i, ex in enumerate(FEW_SHOT_EXAMPLES)\n",
    "    ])\n",
    "    return examples_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Story Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_story(facts: str, format_type: str = \"one-liner\", temperature: float = 0.8, max_tokens: int = 500):\n",
    "    \"\"\"\n",
    "    Generate an absurdist story using Llama-3.1-8B via Hugging Face pipeline.\n",
    "    \n",
    "    Args:\n",
    "        facts: The mundane input facts to transform\n",
    "        format_type: Output format (\"one-liner\", \"poem\", \"dialogue\", \"short-story\")\n",
    "        temperature: Creativity level (0.0-2.0, higher = weirder)\n",
    "        max_tokens: Maximum length of output\n",
    "    \n",
    "    Returns:\n",
    "        Generated absurdist narrative\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build the user prompt with few-shot examples\n",
    "    user_prompt = f\"\"\"{format_few_shot_examples()}\n",
    "\n",
    "Now, transform these facts into an absurdist narrative:\n",
    "\n",
    "Input Facts: {facts}\n",
    "Format: {format_type}\n",
    "\n",
    "Output:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Format messages for the pipeline\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        \n",
    "        # Generate response using pipeline\n",
    "        outputs = pipe(\n",
    "            messages,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            return_full_text=False  # Only return the generated text, not the prompt\n",
    "        )\n",
    "        \n",
    "        # Extract the generated text\n",
    "        response = outputs[0][\"generated_text\"].strip()\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error generating story: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_story(facts: str, format_type: str = \"one-liner\", temperature: float = 0.8, max_tokens: int = 500):\n",
    "    \"\"\"\n",
    "    Generate an absurdist story using Llama-3.1-8B via Hugging Face pipeline.\n",
    "    \n",
    "    Args:\n",
    "        facts: The mundane input facts to transform\n",
    "        format_type: Output format (\"one-liner\", \"poem\", \"dialogue\", \"short-story\")\n",
    "        temperature: Creativity level (0.0-2.0, higher = weirder)\n",
    "        max_tokens: Maximum length of output\n",
    "    \n",
    "    Returns:\n",
    "        Generated absurdist narrative\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build the user prompt with few-shot examples\n",
    "    user_prompt = f\"\"\"{format_few_shot_examples()}\n",
    "\n",
    "Now, transform these facts into an absurdist narrative:\n",
    "\n",
    "Input Facts: {facts}\n",
    "Format: {format_type}\n",
    "\n",
    "Output:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Format messages for the pipeline\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        \n",
    "        # Generate response using pipeline\n",
    "        outputs = pipe(\n",
    "            messages,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            pad_token_id=pipe.tokenizer.eos_token_id  # Fix the pad_token warning\n",
    "        )\n",
    "        \n",
    "        # Extract the generated text - handle different output formats\n",
    "        try:\n",
    "            # Try to get the generated_text field\n",
    "            if isinstance(outputs, list) and len(outputs) > 0:\n",
    "                output = outputs[0]\n",
    "                if isinstance(output, dict) and \"generated_text\" in output:\n",
    "                    # If it's a dict with generated_text, extract it\n",
    "                    generated = output[\"generated_text\"]\n",
    "                    # If it's a list of messages (chat format), get the last assistant message\n",
    "                    if isinstance(generated, list):\n",
    "                        # Find the last assistant message\n",
    "                        for msg in reversed(generated):\n",
    "                            if isinstance(msg, dict) and msg.get(\"role\") == \"assistant\":\n",
    "                                response = msg.get(\"content\", \"\").strip()\n",
    "                                break\n",
    "                        else:\n",
    "                            response = str(generated[-1]) if generated else \"\"\n",
    "                    else:\n",
    "                        response = str(generated).strip()\n",
    "                else:\n",
    "                    response = str(output).strip()\n",
    "            else:\n",
    "                response = str(outputs).strip()\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Fallback: just convert to string\n",
    "            response = str(outputs)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        error_details = traceback.format_exc()\n",
    "        return f\"Error generating story: {str(e)}\\n\\nDetails:\\n{error_details}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_story(facts: str, format_type: str = \"one-liner\", temperature: float = 0.8, max_tokens: int = 500):\n    \"\"\"\n    Generate an absurdist story using Hugging Face pipeline.\n    \n    Args:\n        facts: The mundane input facts to transform\n        format_type: Output format (\"one-liner\", \"poem\", \"dialogue\", \"short-story\")\n        temperature: Creativity level (0.0-2.0, higher = weirder)\n        max_tokens: Maximum length of output\n    \n    Returns:\n        Generated absurdist narrative\n    \"\"\"\n    \n    # Build the user prompt with few-shot examples\n    user_prompt = f\"\"\"{format_few_shot_examples()}\n\nNow, transform these facts into an absurdist narrative:\n\nInput Facts: {facts}\nFormat: {format_type}\n\nOutput:\"\"\"\n    \n    try:\n        # Format messages for the pipeline\n        messages = [\n            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ]\n        \n        # Generate response using pipeline with proper padding\n        outputs = pipe(\n            messages,\n            max_new_tokens=max_tokens,\n            temperature=temperature,\n            top_p=0.95,\n            do_sample=True,\n            pad_token_id=pipe.tokenizer.pad_token_id,\n            eos_token_id=pipe.tokenizer.eos_token_id\n        )\n        \n        # Extract the generated text - handle different output formats\n        if isinstance(outputs, list) and len(outputs) > 0:\n            output = outputs[0]\n            if isinstance(output, dict) and \"generated_text\" in output:\n                generated = output[\"generated_text\"]\n                # If it's a list of messages (chat format), get the last assistant message\n                if isinstance(generated, list):\n                    # Find the last assistant message\n                    for msg in reversed(generated):\n                        if isinstance(msg, dict) and msg.get(\"role\") == \"assistant\":\n                            return msg.get(\"content\", \"\").strip()\n                    # Fallback: return the last message content\n                    if generated and isinstance(generated[-1], dict):\n                        return generated[-1].get(\"content\", str(generated[-1])).strip()\n                    return str(generated[-1]) if generated else \"\"\n                else:\n                    return str(generated).strip()\n            else:\n                return str(output).strip()\n        \n        return str(outputs).strip()\n    \n    except Exception as e:\n        import traceback\n        error_details = traceback.format_exc()\n        return f\"Error generating story: {str(e)}\\n\\nDetails:\\n{error_details}\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for scenario in TEST_SCENARIOS:\n",
    "    print(\"=\"*80)\n",
    "    print(f\"SCENARIO: {scenario['name']}\")\n",
    "    print(f\"Facts: {scenario['facts']}\")\n",
    "    print(f\"Format: {scenario['format']}\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Generate stories at different temperatures\n",
    "    for temp in TEMPERATURE_SETTINGS:\n",
    "        print(f\"üå°Ô∏è  Temperature {temp} (Weirdness Level: {'Medium' if temp < 1.0 else 'High'})\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        story = generate_story(\n",
    "            facts=scenario['facts'],\n",
    "            format_type=scenario['format'],\n",
    "            temperature=temp\n",
    "        )\n",
    "        \n",
    "        results[temp] = story\n",
    "        print(story)\n",
    "        print()\n",
    "        \n",
    "        # Small delay to avoid rate limiting\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Story Generator (Interactive)\n",
    "### Use this cell to generate individual stories with custom inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize these values\n",
    "custom_facts = \"Your custom facts here\"\n",
    "custom_format = \"one-liner\"  # Options: \"one-liner\", \"poem\", \"dialogue\", \"short-story\"\n",
    "custom_temperature = 1.0\n",
    "\n",
    "# Generate\n",
    "result = generate_story(custom_facts, custom_format, custom_temperature)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing for Friend Group\n",
    "### Generate personalized stories for multiple people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your friend group's personalized facts here\n",
    "FRIEND_FACTS = {\n",
    "    \"Alex\": \"Always forgets to water their plants but somehow they keep thriving.\",\n",
    "    \"Sam\": \"Has 47 unread books on their nightstand and keeps buying more.\",\n",
    "    \"Jordan\": \"Insists on making sourdough from scratch but the starter has become sentient.\",\n",
    "    \"Taylor\": \"Collects vintage spoons but has never used any of them.\"\n",
    "}\n",
    "\n",
    "print(\"üé≠ FRIEND GROUP ABSURDIST PROFILES\\n\")\n",
    "\n",
    "for name, facts in FRIEND_FACTS.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚ú® {name}'s Absurdist Portrait\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    story = generate_story(facts, format_type=\"poem\", temperature=1.0)\n",
    "    print(story)\n",
    "    print()\n",
    "    \n",
    "    time.sleep(1)  # Rate limiting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}