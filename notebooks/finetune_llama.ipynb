{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Llama-3.1-8B on AO3 Fragments\n",
    "\n",
    "This notebook fine-tunes Llama-3.1-8B on 133k emotionally intense story fragments using LoRA.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab Pro (T4 or A100 GPU)\n",
    "- Upload train.jsonl, val.jsonl, test.jsonl to Colab\n",
    "- Hugging Face account (for model access)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q -U transformers datasets accelerate peft bitsandbytes trl wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face (needed for Llama-3.1 access)\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Login to Weights & Biases for experiment tracking (optional)\n",
    "# import wandb\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Training Data\n",
    "\n",
    "Upload these files to Colab:\n",
    "1. `train.jsonl` (~106k fragments)\n",
    "2. `val.jsonl` (~13k fragments)\n",
    "3. `test.jsonl` (~13k fragments)\n",
    "\n",
    "Click the folder icon on the left sidebar, then upload the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "path = 'drive/MyDrive/NEW APP/'\n",
    "\n",
    "# Load JSONL files\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\n",
    "        \"train\": path + \"train.jsonl\",\n",
    "        \"validation\": path + \"val.jsonl\",\n",
    "        \"test\": path + \"test.jsonl\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(dataset['train']):,} examples\")\n",
    "print(f\"Validation: {len(dataset['validation']):,} examples\")\n",
    "print(f\"Test: {len(dataset['test']):,} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model with 4-bit Quantization\n",
    "\n",
    "We use 4-bit quantization to fit Llama-3.1-8B in GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "\n",
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "print(\"✓ Model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure LoRA\n",
    "\n",
    "LoRA allows efficient fine-tuning by training only a small number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Prepare model for LoRA training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank of LoRA matrices\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[  # Which layers to apply LoRA to\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"✓ LoRA configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Tokenize text\n",
    "    outputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=80,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # For causal LM, labels are the same as input_ids\n",
    "    outputs[\"labels\"] = outputs[\"input_ids\"].copy()\n",
    "\n",
    "    return outputs\n",
    "\n",
    "# Tokenize all splits\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "print(\"✓ Data tokenized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# FASTER training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama-3.1-ao3-fragments\",\n",
    "    num_train_epochs=2,  # Changed from 3 to 2\n",
    "    per_device_train_batch_size=8,  # Changed from 4 to 8\n",
    "    per_device_eval_batch_size=8,  # Changed from 4 to 8\n",
    "    gradient_accumulation_steps=2,  # Changed from 4 to 2\n",
    "\n",
    "    # Learning rate\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "\n",
    "    # Optimization\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=0.3,\n",
    "\n",
    "    # Logging and evaluation - REDUCED\n",
    "    logging_steps=200,  # Changed from 50\n",
    "    eval_strategy=\"epoch\",  # Changed from steps - only eval at end of each epoch\n",
    "    save_strategy=\"epoch\",  # Changed from steps\n",
    "    save_total_limit=2,  # Changed from 3\n",
    "\n",
    "    # Performance\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    group_by_length=True,\n",
    "    dataloader_num_workers=2,  # Added for faster data loading\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapter\n",
    "model.save_pretrained(\"./llama-3.1-ao3-lora-adapter\")\n",
    "tokenizer.save_pretrained(\"./llama-3.1-ao3-lora-adapter\")\n",
    "\n",
    "print(\"✓ Model saved to ./llama-3.1-ao3-lora-adapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_results = trainer.evaluate(tokenized_dataset[\"test\"])\n",
    "\n",
    "print(\"Test results:\")\n",
    "for key, value in test_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Generation\n",
    "\n",
    "Generate some sample stories to see how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples\n",
    "model.eval()\n",
    "\n",
    "prompts = [\n",
    "    \"Sarah and Emma\",\n",
    "    \"The moment when\",\n",
    "    \"In the darkness,\",\n",
    "    \"Alex couldn't believe\",\n",
    "    \"They finally\"\n",
    "]\n",
    "\n",
    "print(\"Generated stories:\\n\" + \"=\"*80)\n",
    "\n",
    "for prompt in prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        temperature=0.9,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {generated_text}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Model\n",
    "\n",
    "Download the LoRA adapter to use locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the adapter folder\n",
    "!zip -r llama-3.1-ao3-lora-adapter.zip llama-3.1-ao3-lora-adapter\n",
    "\n",
    "# Download the zip file\n",
    "from google.colab import files\n",
    "files.download('llama-3.1-ao3-lora-adapter.zip')\n",
    "\n",
    "print(\"✓ Download started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "After downloading the adapter, you can load it locally:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, \"./llama-3.1-ao3-lora-adapter\")\n",
    "\n",
    "# Generate\n",
    "inputs = tokenizer(\"Sarah and Emma\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
