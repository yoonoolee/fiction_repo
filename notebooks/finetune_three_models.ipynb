{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Fine-tune 3 Models with Llama-3.1-8B-Instruct\n",
    "\n",
    "Trains 3 models sequentially:\n",
    "1. **Combined**: Balanced dataset (10k one-liners + 10k short stories)\n",
    "2. **One-Liner**: Full dataset (~2k)\n",
    "3. **Short-Story**: Full dataset (~50k)\n",
    "\n",
    "**Prerequisites:** Run `prepare_training_data.ipynb` first to generate JSONL files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "TRAIN_DIR = Path(\"../data/train\")\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "print(f\"Using model: {MODEL_NAME}\")\n",
    "print(f\"Training data directory: {TRAIN_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Define Fine-tuning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_model(dataset_name, output_name, num_epochs=3):\n",
    "    \"\"\"\n",
    "    Fine-tune Llama-3.1-8B-Instruct with LoRA.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name of dataset (e.g., 'combined', 'one_liner', 'short_story')\n",
    "        output_name: Name for output directory (e.g., 'llama-8b-combined')\n",
    "        num_epochs: Number of training epochs (default: 3)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Starting training: {output_name}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    # Load model\n",
    "    print(\"Loading model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    # Configure LoRA\n",
    "    print(\"Configuring LoRA...\")\n",
    "    lora_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=128,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    # Load datasets\n",
    "    print(f\"Loading {dataset_name} dataset...\")\n",
    "    train_dataset = load_dataset('json', data_files=str(TRAIN_DIR / f\"{dataset_name}_train.jsonl\"), split='train')\n",
    "    val_dataset = load_dataset('json', data_files=str(TRAIN_DIR / f\"{dataset_name}_val.jsonl\"), split='train')\n",
    "    \n",
    "    print(f\"Train examples: {len(train_dataset):,}\")\n",
    "    print(f\"Val examples: {len(val_dataset):,}\")\n",
    "    \n",
    "    # Format and tokenize\n",
    "    print(\"Formatting and tokenizing...\")\n",
    "    def format_chat_template(example):\n",
    "        messages = example['messages']\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "        return {\"text\": text}\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        outputs = tokenizer(examples[\"text\"], truncation=True, max_length=1024, padding=\"max_length\")\n",
    "        outputs[\"labels\"] = outputs[\"input_ids\"].copy()\n",
    "        return outputs\n",
    "    \n",
    "    train_dataset = train_dataset.map(format_chat_template, remove_columns=train_dataset.column_names)\n",
    "    val_dataset = val_dataset.map(format_chat_template, remove_columns=val_dataset.column_names)\n",
    "    \n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "    val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "    \n",
    "    # Training arguments\n",
    "    output_dir = f\"./models/{output_name}\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        learning_rate=2e-4,\n",
    "        warmup_ratio=0.03,\n",
    "        logging_steps=25,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        bf16=True,\n",
    "        report_to=\"none\",\n",
    "        gradient_checkpointing=True,\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\nStarting training...\\n\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save\n",
    "    print(f\"\\nSaving model to {output_dir}...\")\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    print(f\"\\nâœ“ Completed: {output_name}\\n\")\n",
    "    \n",
    "    # Clean up VRAM\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Train All 3 Models Sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Combined (balanced dataset)\n",
    "model_1 = finetune_model(\n",
    "    dataset_name=\"combined\",\n",
    "    output_name=\"llama-8b-combined\",\n",
    "    num_epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: One-liner only\n",
    "model_2 = finetune_model(\n",
    "    dataset_name=\"one_liner\",\n",
    "    output_name=\"llama-8b-one-liner\",\n",
    "    num_epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Short-story only\n",
    "model_3 = finetune_model(\n",
    "    dataset_name=\"short_story\",\n",
    "    output_name=\"llama-8b-short-story\",\n",
    "    num_epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL MODELS TRAINED SUCCESSFULLY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n1. {model_1}\")\n",
    "print(f\"2. {model_2}\")\n",
    "print(f\"3. {model_3}\")\n",
    "print(\"\\nNext steps: Test the models with inference code.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
